{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOKUs9s8yJSZ8AKRHw3GsSb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"1ab7053b","executionInfo":{"status":"ok","timestamp":1754883245994,"user_tz":-480,"elapsed":15,"user":{"displayName":"Lester Anthony Jr. Sityar","userId":"05503413341659627231"}}},"source":["import json\n","\n","with open('/content/baseline_validation_results.json', 'r') as f:\n","    prompt_engineered_results = json.load(f)\n","\n","with open('/content/agentic_validation_results.json', 'r') as f:\n","    agentic_results = json.load(f)"],"execution_count":3,"outputs":[]},{"cell_type":"code","source":["if prompt_engineered_results is not None:\n","    print(\"First 3 results from Prompt-Engineered Judge:\")\n","    for i, entry in enumerate(prompt_engineered_results[:3]):\n","        display(entry)\n","else:\n","    print(\"Prompt-engineered results not loaded.\")\n","\n","if agentic_results is not None:\n","    print(\"\\nFirst 3 results from Agentic Judge:\")\n","    for i, entry in enumerate(agentic_results[:3]):\n","        display(entry)\n","else:\n","    print(\"Agentic results not loaded.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"collapsed":true,"id":"7ZY-eoNw5bJn","executionInfo":{"status":"ok","timestamp":1754883246013,"user_tz":-480,"elapsed":18,"user":{"displayName":"Lester Anthony Jr. Sityar","userId":"05503413341659627231"}},"outputId":"b7ca08e4-0bab-46e5-de05-33e3a5f7b432"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["First 3 results from Prompt-Engineered Judge:\n"]},{"output_type":"display_data","data":{"text/plain":["{'entry_index': 0,\n"," 'source_text': 'The children laughed and played under the afternoon sun.',\n"," 'target_text': 'Ang mga bata ay nagtawanan at naglaro sa ilalim ng hapon na araw.',\n"," 'evaluation_result': {'scores': {'accuracy': 5,\n","   'tone_and_style': 5,\n","   'nuance_and_metaphor': 5,\n","   'fluency': 5,\n","   'overall': 5},\n","  'explanation': \"The translation accurately preserves the core meaning of the source text, capturing the scene of children playing under the afternoon sun. The translation's tone and style are perfectly matched, conveying a sense of carefree joy and innocence. The nuances of the original text, such as the use of 'afternoon sun' to evoke a sense of warmth and comfort, are effectively translated. The translation also maintains a natural and fluent tone, with no awkward phrasing or grammatical errors. The translation's use of 'ilalim ng hapon na araw' to convey the time of day is a good cultural equivalent, and the phrase 'nagtawanan at naglaro' effectively conveys the children's playful activities. Overall, the translation is a perfect representation of the original text, with no significant errors or omissions.\"},\n"," 'human_score': 4.0,\n"," 'human_explanation': 'Accurate, fluent, and natural translation. Captures the tone and meaning well.'}"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["{'entry_index': 1,\n"," 'source_text': 'She took a break to gather her thoughts.',\n"," 'target_text': 'Nagpahinga siya para mag-isip-isip.',\n"," 'evaluation_result': {'scores': {'accuracy': 5,\n","   'tone_and_style': 5,\n","   'nuance_and_metaphor': 5,\n","   'fluency': 5,\n","   'overall': 5},\n","  'explanation': \"The translation 'Nagpahinga siya para mag-isip-isip' accurately conveys the core meaning of the source text, which is that the subject is taking a break to gather her thoughts. The translation uses the correct Filipino phrases to convey the idea of 'gathering thoughts', which is'mag-isip-isip' in Filipino. The tone and style of the translation are also perfectly captured, as it maintains the formal and neutral tone of the original text. The translation effectively handles the nuances and metaphors, using the correct Filipino idioms to convey the idea of taking a break. The translation reads naturally and fluently, with no awkward phrasing or grammatical errors. Overall, the translation is perfectly accurate, captures the tone and style, and effectively conveys the nuances and metaphors, making it a high-quality translation.\"},\n"," 'human_score': 4.0,\n"," 'human_explanation': 'The translation is accurate. It was able to capture the meaning of the idiom \"gather her thoughts\", instead of translating it literally to \"kumuha ng ideya\"'}"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["{'entry_index': 2,\n"," 'source_text': 'The algorithm efficiently identifies patterns in large datasets.',\n"," 'target_text': 'Mabisang kinikilala ng algoritmo ang mga pattern sa malalaking dataset.',\n"," 'evaluation_result': {'scores': {'accuracy': 5,\n","   'tone_and_style': 5,\n","   'nuance_and_metaphor': 5,\n","   'fluency': 5,\n","   'overall': 5},\n","  'explanation': \"The translation accurately preserves the core meaning of the source text, using the correct Filipino words to convey the concept of an algorithm identifying patterns in large datasets. The tone and style of the translation are perfectly captured, with the use of the word'mabisang' (meaning 'efficiently' or 'effectively') to convey the algorithm's ability to identify patterns. The translation also effectively handles the nuances and metaphors present in the source text, using the phrase'mga pattern' to convey the idea of patterns in datasets. The translation reads naturally and fluently, with no awkward phrasing or grammatical errors. Overall, the translation is a perfect representation of the source text, capturing its essence and conveying its meaning accurately and effectively.\"},\n"," 'human_score': 3.0,\n"," 'human_explanation': 'The translation of \"identifies\" as \"kinikilala\" in the context of the sentence is not very accurate. The source text was translated literally.'}"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","First 3 results from Agentic Judge:\n"]},{"output_type":"display_data","data":{"text/plain":["{'entry_index': 0,\n"," 'source_text': 'The children laughed and played under the afternoon sun.',\n"," 'target_text': 'Ang mga bata ay nagtawanan at naglaro sa ilalim ng hapon na araw.',\n"," 'evaluation_result': {'scores': {'accuracy': 5,\n","   'tone_and_style': 5,\n","   'nuance_and_metaphor': 5,\n","   'fluency': 5,\n","   'overall': 5},\n","  'explanation': \"Based on my analysis, I chose the category of (Score: 5) for the Tone & Style Analysis. This is because the tones of the English source text and the Filipino translation are a perfect match, conveying a sense of warmth and friendliness. The use of simple sentence structures and the focus on the children's playful activities in both texts maintain a similar informal and conversational tone. The poetic quality of the original text is also preserved in the translation, with the phrase 'ilalim ng hapon na araw' evoking a sense of serenity and happiness. The connotation of the Filipino word 'hapon' (afternoon) is neutral, which does not affect the overall tone of the translation. Overall, the tone and style of the translation accurately capture the essence of the original text, making it a perfect match.\"},\n"," 'human_score': 4.0,\n"," 'human_explanation': 'Accurate, fluent, and natural translation. Captures the tone and meaning well.'}"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["{'entry_index': 1,\n"," 'source_text': 'She took a break to gather her thoughts.',\n"," 'target_text': 'Nagpahinga siya para mag-isip-isip.',\n"," 'evaluation_result': {'scores': {'accuracy': 2,\n","   'tone_and_style': 1,\n","   'nuance_and_metaphor': 2,\n","   'fluency': 3,\n","   'overall': 2.2},\n","  'explanation': \"Based on my analysis, I chose the category of mismatched tones. The tone of the English source text is informal and conversational, while the tone of the Filipino translation is more formal and literal. The translation attempts to convey the same meaning, but the tone is not as relaxed or casual as the original text. The use of the verb 'nagpahinga' (to sleep or rest) implies a more prolonged or continuous period of rest, which may not accurately convey the nuance of the original phrase. The addition of 'para mag-isip-isip' (to think) attempts to convey the idea of gathering thoughts, but it's a more formal and structured phrase, which may not be as conversational or relaxed as the original text. Overall, the tones of the two texts do not match, and the translation falls short in conveying the nuance and tone of the original phrase.\"},\n"," 'human_score': 4.0,\n"," 'human_explanation': 'The translation is accurate. It was able to capture the meaning of the idiom \"gather her thoughts\", instead of translating it literally to \"kumuha ng ideya\"'}"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["{'entry_index': 2,\n"," 'source_text': 'The algorithm efficiently identifies patterns in large datasets.',\n"," 'target_text': 'Mabisang kinikilala ng algoritmo ang mga pattern sa malalaking dataset.',\n"," 'evaluation_result': {'scores': {'accuracy': 5,\n","   'tone_and_style': 4,\n","   'nuance_and_metaphor': 3,\n","   'fluency': 4,\n","   'overall': 4.1},\n","  'explanation': \"Based on my analysis, I chose the category of 'Tone & Style Scoring Guide' with a score of 4. The tones of the English source text and the Filipino translation are broadly similar, but with minor differences. The translation maintains the same level of formality and technicality as the source text, which is important for conveying the same meaning and information. The slight difference in tone between the two texts is mainly due to the differences in language and cultural context, but the overall tone remains consistent. The translation's use of the phrase 'Mabisang kinikilala' gives it a slightly more dynamic and active tone compared to the passive voice of the English source text, but this difference is minor and does not fundamentally change the tone. Overall, the translation's tone is formal and objective, suitable for a technical or academic context.\"},\n"," 'human_score': 3.0,\n"," 'human_explanation': 'The translation of \"identifies\" as \"kinikilala\" in the context of the sentence is not very accurate. The source text was translated literally.'}"]},"metadata":{}}]},{"cell_type":"code","source":["from scipy.stats import spearmanr\n","import pandas as pd\n","\n","if prompt_engineered_results is None or agentic_results is None:\n","    print(\"Data not loaded. Cannot calculate Spearman's Rank Correlation.\")\n","else:\n","    # --- Calculate Spearman's Rank Correlation (Prompt-Engineered vs Human) ---\n","    human_scores_pe = []\n","    prompt_engineered_model_scores_filtered = []\n","    for entry in prompt_engineered_results:\n","        human_score = entry.get('human_score')\n","        overall_score = entry.get('evaluation_result', {}).get('scores', {}).get('overall')\n","        if human_score is not None and overall_score is not None:\n","            human_scores_pe.append(human_score)\n","            prompt_engineered_model_scores_filtered.append(overall_score)\n","\n","    min_len_pe = min(len(human_scores_pe), len(prompt_engineered_model_scores_filtered))\n","    human_scores_pe = human_scores_pe[:min_len_pe]\n","    prompt_engineered_model_scores_filtered = prompt_engineered_model_scores_filtered[:min_len_pe]\n","\n","    if len(human_scores_pe) > 1 and len(prompt_engineered_model_scores_filtered) > 1:\n","        spearman_pe, _ = spearmanr(human_scores_pe, prompt_engineered_model_scores_filtered)\n","        print(f\"Spearman's Rank Correlation (Prompt-Engineered vs Human): {spearman_pe:.4f}\")\n","    else:\n","        print(\"Insufficient data points to calculate Spearman's Rank Correlation for Prompt-Engineered model vs Human.\")\n","\n","    # --- Calculate Spearman's Rank Correlation (Agentic vs Human) ---\n","    human_scores_agentic = []\n","    agentic_model_scores_filtered = []\n","    for entry in agentic_results:\n","        human_score = entry.get('human_score')\n","        overall_score = entry.get('evaluation_result', {}).get('scores', {}).get('overall')\n","        if human_score is not None and overall_score is not None:\n","            human_scores_agentic.append(human_score)\n","            agentic_model_scores_filtered.append(overall_score)\n","\n","    min_len_agentic = min(len(human_scores_agentic), len(agentic_model_scores_filtered))\n","    human_scores_agentic = human_scores_agentic[:min_len_agentic]\n","    agentic_model_scores_filtered = agentic_model_scores_filtered[:min_len_agentic]\n","\n","    if len(human_scores_agentic) > 1 and len(agentic_model_scores_filtered) > 1:\n","        spearman_agentic, _ = spearmanr(human_scores_agentic, agentic_model_scores_filtered)\n","        print(f\"Spearman's Rank Correlation (Agentic vs Human): {spearman_agentic:.4f}\")\n","    else:\n","        print(\"Insufficient data points to calculate Spearman's Rank Correlation for Agentic model vs Human.\")\n","\n","    # --- Calculate Spearman's Rank Correlation (Prompt-Engineered vs Agentic) ---\n","    prompt_engineered_overall_scores = {}\n","    for entry in prompt_engineered_results:\n","        overall_score = entry.get('evaluation_result', {}).get('scores', {}).get('overall')\n","        if overall_score is not None:\n","            prompt_engineered_overall_scores[entry.get('entry_index')] = overall_score\n","\n","    agentic_overall_scores = {}\n","    for entry in agentic_results:\n","        overall_score = entry.get('evaluation_result', {}).get('scores', {}).get('overall')\n","        if overall_score is not None:\n","            agentic_overall_scores[entry.get('entry_index')] = overall_score\n","\n","    common_indices = set(prompt_engineered_overall_scores.keys()).intersection(set(agentic_overall_scores.keys()))\n","    common_indices = sorted(list(common_indices))\n","\n","    pe_scores_for_correlation = [prompt_engineered_overall_scores[idx] for idx in common_indices]\n","    agentic_scores_for_correlation = [agentic_overall_scores[idx] for idx in common_indices]\n","\n","    if len(pe_scores_for_correlation) > 1 and len(agentic_scores_for_correlation) > 1:\n","        spearman_pe_agentic, _ = spearmanr(pe_scores_for_correlation, agentic_scores_for_correlation)\n","        print(f\"Spearman's Rank Correlation (Prompt-Engineered vs Agentic): {spearman_pe_agentic:.4f}\")\n","    else:\n","        print(\"Insufficient common data points to calculate Spearman's Rank Correlation between Prompt-Engineered and Agentic models.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VXs293rX5i7Y","executionInfo":{"status":"ok","timestamp":1754883247063,"user_tz":-480,"elapsed":1049,"user":{"displayName":"Lester Anthony Jr. Sityar","userId":"05503413341659627231"}},"outputId":"d8d795a4-f5b4-4d2c-d109-c976241f05cb"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Spearman's Rank Correlation (Prompt-Engineered vs Human): 0.4110\n","Spearman's Rank Correlation (Agentic vs Human): 0.1376\n","Spearman's Rank Correlation (Prompt-Engineered vs Agentic): 0.2367\n"]}]},{"cell_type":"code","source":["import numpy as np\n","\n","if prompt_engineered_results is None or agentic_results is None:\n","    print(\"Data not loaded. Cannot calculate variance.\")\n","else:\n","    # Extract overall scores for variance calculation, filtering out None values\n","    prompt_engineered_overall_scores = [\n","        entry.get('evaluation_result', {}).get('scores', {}).get('overall')\n","        for entry in prompt_engineered_results\n","        if entry.get('evaluation_result', {}).get('scores', {}).get('overall') is not None\n","    ]\n","\n","    agentic_overall_scores = [\n","        entry.get('evaluation_result', {}).get('scores', {}).get('overall')\n","        for entry in agentic_results\n","        if entry.get('evaluation_result', {}).get('scores', {}).get('overall') is not None\n","    ]\n","\n","    # Extract human scores for variance calculation, filtering out None values\n","    human_scores = [\n","        entry.get('human_score')\n","        for entry in prompt_engineered_results\n","        if entry.get('human_score') is not None\n","    ]\n","\n","    if len(prompt_engineered_overall_scores) > 1:\n","        variance_pe = np.var(prompt_engineered_overall_scores)\n","        print(f\"Variance of Overall Scores (Prompt-Engineered): {variance_pe:.4f}\")\n","    else:\n","        print(\"Insufficient data points to calculate variance for Prompt-Engineered model.\")\n","\n","    if len(agentic_overall_scores) > 1:\n","        variance_agentic = np.var(agentic_overall_scores)\n","        print(f\"Variance of Overall Scores (Agentic): {variance_agentic:.4f}\")\n","    else:\n","        print(\"Insufficient data points to calculate variance for Agentic model.\")\n","\n","    if len(human_scores) > 1:\n","        variance_human = np.var(human_scores)\n","        print(f\"Variance of Human Scores: {variance_human:.4f}\")\n","    else:\n","        print(\"Insufficient data points to calculate variance for Human scores.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yOmwsy3I5wu0","executionInfo":{"status":"ok","timestamp":1754883247071,"user_tz":-480,"elapsed":6,"user":{"displayName":"Lester Anthony Jr. Sityar","userId":"05503413341659627231"}},"outputId":"2dc4f7b9-ba02-4d04-a654-86615e7a657c"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Variance of Overall Scores (Prompt-Engineered): 0.1696\n","Variance of Overall Scores (Agentic): 0.7281\n","Variance of Human Scores: 1.5069\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FTj5CU_9BG","executionInfo":{"status":"ok","timestamp":1754883247082,"user_tz":-480,"elapsed":11,"user":{"displayName":"Lester Anthony Jr. Sityar","userId":"05503413341659627231"}},"outputId":"e88eda46-e00c-4a05-b0ba-24ca3cee0d3f"},"source":["import numpy as np\n","\n","if prompt_engineered_results is None or agentic_results is None:\n","    print(\"Data not loaded. Cannot calculate averages.\")\n","else:\n","    # Function to calculate average scores for specific criteria\n","    def calculate_average_scores(results):\n","        scores = {\n","            'accuracy': [],\n","            'tone_and_style': [],\n","            'nuance_and_metaphor': [],\n","            'fluency': [],\n","            'overall': [] # Add overall score\n","        }\n","        for entry in results:\n","            evaluation_result = entry.get('evaluation_result', {})\n","            scores_dict = evaluation_result.get('scores', {})\n","            for criterion in scores.keys():\n","                score = scores_dict.get(criterion)\n","                if score is not None:\n","                    scores[criterion].append(score)\n","        average_scores = {}\n","        for criterion, score_list in scores.items():\n","            if score_list:\n","                average_scores[criterion] = np.mean(score_list)\n","            else:\n","                average_scores[criterion] = \"N/A\" # Handle cases with no valid scores\n","        return average_scores\n","\n","    # Calculate and print averages for Prompt-Engineered\n","    print(\"Average Scores (Prompt-Engineered):\")\n","    average_pe = calculate_average_scores(prompt_engineered_results)\n","    for criterion, avg_score in average_pe.items():\n","        print(f\"  {criterion.replace('_', ' ').title()}: {avg_score:.4f}\" if isinstance(avg_score, float) else f\"  {criterion.replace('_', ' ').title()}: {avg_score}\")\n","\n","    print(\"\\nAverage Scores (Agentic):\")\n","    average_agentic = calculate_average_scores(agentic_results)\n","    for criterion, avg_score in average_agentic.items():\n","        print(f\"  {criterion.replace('_', ' ').title()}: {avg_score:.4f}\" if isinstance(avg_score, float) else f\"  {criterion.replace('_', ' ').title()}: {avg_score}\")"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Average Scores (Prompt-Engineered):\n","  Accuracy: 4.5965\n","  Tone And Style: 4.5965\n","  Nuance And Metaphor: 4.5965\n","  Fluency: 4.8772\n","  Overall: 4.6667\n","\n","Average Scores (Agentic):\n","  Accuracy: 4.5893\n","  Tone And Style: 3.8571\n","  Nuance And Metaphor: 4.0714\n","  Fluency: 4.5000\n","  Overall: 4.2339\n"]}]},{"cell_type":"code","source":["import json\n","import numpy as np\n","import os\n","\n","# Load the variance subset results files\n","file_path_baseline_subset = os.path.join('/content/', 'baseline_variance_subset_results.json')\n","file_path_agentic_subset = os.path.join('/content/', 'agentic_variance_subset_results.json')\n","\n","baseline_subset_results = None\n","agentic_subset_results = None\n","\n","try:\n","    with open(file_path_baseline_subset, 'r') as f:\n","        baseline_subset_results = json.load(f)\n","except FileNotFoundError:\n","    print(f\"Error: {file_path_baseline_subset} not found.\")\n","\n","try:\n","    with open(file_path_agentic_subset, 'r') as f:\n","        agentic_subset_results = json.load(f)\n","except FileNotFoundError:\n","    print(f\"Error: {file_path_agentic_subset} not found.\")\n","\n","\n","if baseline_subset_results is None and agentic_subset_results is None:\n","    print(\"Neither variance subset data file was loaded. Cannot calculate variance.\")\n","else:\n","    # Function to extract overall scores and calculate variance\n","    def calculate_overall_variance(results, model_name):\n","        if results is None:\n","            print(f\"Data for {model_name} not loaded. Cannot calculate variance.\")\n","            return\n","\n","        overall_scores = [\n","            entry.get('evaluation_result', {}).get('scores', {}).get('overall')\n","            for entry in results\n","            if entry.get('evaluation_result', {}).get('scores', {}).get('overall') is not None\n","        ]\n","\n","        if len(overall_scores) > 1:\n","            variance = np.var(overall_scores)\n","            print(f\"Variance of Overall Scores ({model_name}): {variance:.4f}\")\n","        else:\n","            print(f\"Insufficient data points to calculate variance for {model_name}.\")\n","\n","    # Calculate and print variance for Baseline subset\n","    calculate_overall_variance(baseline_subset_results, \"Baseline Subset\")\n","\n","    # Calculate and print variance for Agentic subset\n","    calculate_overall_variance(agentic_subset_results, \"Agentic Subset\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0iiVFSiCL__P","executionInfo":{"status":"ok","timestamp":1754883247095,"user_tz":-480,"elapsed":12,"user":{"displayName":"Lester Anthony Jr. Sityar","userId":"05503413341659627231"}},"outputId":"11b7e4ee-6382-4867-8884-f8ff7bb315f6"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Insufficient data points to calculate variance for Baseline Subset.\n","Insufficient data points to calculate variance for Agentic Subset.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"408a9817","executionInfo":{"status":"ok","timestamp":1754883473283,"user_tz":-480,"elapsed":5,"user":{"displayName":"Lester Anthony Jr. Sityar","userId":"05503413341659627231"}},"outputId":"41a3e878-8f1b-4844-ac13-d63c830d9d85"},"source":["import json\n","import numpy as np\n","import os\n","\n","# Load the variance subset results files\n","file_path_baseline_subset = os.path.join('/content/', 'baseline_variance_subset_results.json')\n","file_path_agentic_subset = os.path.join('/content/', 'agentic_variance_subset_results.json')\n","\n","baseline_subset_results = None\n","agentic_subset_results = None\n","\n","try:\n","    with open(file_path_baseline_subset, 'r') as f:\n","        baseline_subset_results = json.load(f)\n","except FileNotFoundError:\n","    print(f\"Error: {file_path_baseline_subset} not found.\")\n","\n","try:\n","    with open(file_path_agentic_subset, 'r') as f:\n","        agentic_subset_results = json.load(f)\n","except FileNotFoundError:\n","    print(f\"Error: {file_path_agentic_subset} not found.\")\n","\n","\n","def calculate_consistency(results, model_name):\n","    if results is None:\n","        print(f\"Data for {model_name} not loaded. Cannot calculate consistency.\")\n","        return None\n","\n","    variances = []\n","    for entry in results:\n","        variance_runs = entry.get('variance_runs')\n","        if variance_runs:\n","            overall_scores = [\n","                run.get('evaluation_result', {}).get('scores', {}).get('overall')\n","                for run in variance_runs\n","                if run.get('evaluation_result', {}).get('scores', {}).get('overall') is not None\n","            ]\n","            if len(overall_scores) > 1:\n","                variances.append(np.var(overall_scores))\n","\n","    if variances:\n","        average_variance = np.mean(variances)\n","        print(f\"Average Variance (Consistency) for {model_name}: {average_variance:.4f}\")\n","        return average_variance\n","    else:\n","        print(f\"Insufficient data points with multiple runs to calculate consistency for {model_name}.\")\n","        return None\n","\n","print(\"Calculating Consistency...\")\n","baseline_consistency = calculate_consistency(baseline_subset_results, \"Baseline Subset\")\n","agentic_consistency = calculate_consistency(agentic_subset_results, \"Agentic Subset\")"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Calculating Consistency...\n","Average Variance (Consistency) for Baseline Subset: 0.0000\n","Average Variance (Consistency) for Agentic Subset: 0.1036\n"]}]}]}